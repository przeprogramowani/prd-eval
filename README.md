# PRD evaluation with promptfoo

This repository compares product requirement documents generated by different model vendors. The TypeScript tooling under `src/` prepares a promptfoo run, executes three judges (GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro), and aggregates their JSON responses into repeatable score reports.

## Prerequisites

- Node.js 18 or newer
- API access for every judge configured in `src/promptfoo.config.ts` (set in `.env`):
  - `OPENAI_API_KEY`
  - `ANTHROPIC_API_KEY`
  - `OPENROUTER_API_KEY`

## Installation

```bash
npm install
```

## Running evaluations

```bash
npm run eval
```

The evaluation script clears `dist/results`, rebuilds the TypeScript sources, disables the promptfoo cache, and runs `dist/evalRunner.js`. Each judge receives the rubric in `prd-job-interview.md` plus the vendor PRDs listed in `src/promptfoo.config.ts`. Outputs are collected in:

- `dist/results/latest.json` – raw promptfoo output
- `dist/results/latest.lite.json` – condensed per-judge scores without prompts
- `dist/results/score-history.csv` – append-only log of every run and criterion score

## Inspecting results locally

```bash
npm run display-scores
```

This command re-parses `latest.lite.json` and prints a Markdown summary table (or writes to the GitHub Actions step summary when running in CI).

## Data layout

- `data/{vendor}/prd.md` contains the full PRD for a model run.
- `data/{vendor}/summary.md` captures the at-a-glance synopsis of the same run.
- Vendor folder names stay in lowercase kebab case (for example `data/sonnet-45/`).

Duplicate an existing vendor folder when starting a new run, prune unused drafts with `rg --files data`, and update `modelDirectories` in `src/promptfoo.config.ts` so evaluators pick up the new PRD.

## Configuration overview

- Prompt template and judge list: `src/promptfoo.config.ts`
- Evaluation orchestrator and result logging: `src/evalRunner.ts`
- Markdown score summariser: `src/resultInterpreter.ts`
- Rubric applied to every run: `prd-job-interview.md`

The prompt template enforces strict JSON output, assigns judge aliases, and instructs models to score five criteria plus an overall verdict. `evalRunner` normalises the JSON (stripping code fences or "thinking" blocks) before recording results in CSV and lite JSON formats.

## Maintenance tips

- Run `npm run build` (and optionally `npm run eval`) before committing to ensure the generated JavaScript is up to date.
- Lint documentation with `npx markdownlint-cli2 "data/**/*.md"` and audit for placeholders using `rg "TODO" data`.
- Never edit `dist/` by hand—rebuild instead so generated artifacts stay traceable.
