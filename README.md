# PRD evaluation with promptfoo

This project compares PRDs generated by different model vendors using an interview rubric defined in `prd-job-interview.md`. Evaluations run through [promptfoo](https://promptfoo.dev) with GPT-5 acting as the judge.

## Prerequisites

- Node.js 18+
- `OPENAI_API_KEY` environment variable with access to GPT-5 (can be set in `.env`)

## Install

```bash
npm install
```

## Build

Compile the TypeScript configuration to `dist/`:

```bash
npm run build
```

## Run the evaluation

```bash
npm run eval
```

This command:

- Loads environment variables from `.env` (via `dotenv`)
- Builds the TypeScript sources
- Runs `dist/evalRunner.js`, which executes `promptfoo eval` and appends a row to `dist/results/score-history.csv`
- Persists full promptfoo output alongside the CSV in `dist/results`

## Configuration overview

- Source config: `src/promptfoo.config.ts`
- Rubric: `prd-job-interview.md`
- PRD inputs: `data/{model}/prd.md`

Each case loads the rubric verbatim, injects the corresponding PRD, and instructs GPT-5 to return structured JSON with scores and rationales across five criteria plus an overall verdict.

## Maintenance tips

- Add new vendor runs by extending `modelDirectories` in `src/promptfoo.config.ts`.
- Rebuild after any edits to TypeScript sources before running the eval.
- Store credentials in `.env` or your shell profile rather than committing them.
- Use `npx markdownlint-cli2 "data/**/*.md"` to lint PRDs if Node tooling is available.
